{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07cfd3ef",
   "metadata": {},
   "source": [
    "# Multi-Variable Phenotype Clustering Analysis\n",
    "## Chen 2004 Yeast Cell Cycle Model - Whole Network Phenotypes\n",
    "\n",
    "This notebook implements the second definition of phenotype that considers the whole gene regulatory network rather than a single molecule. We use BIRCH clustering to group mn-dimensional points representing the full system output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fe330de",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmultiprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmultiprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pool\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Birch\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m silhouette_score, pairwise_distances\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# === IMPORTS ===\n",
    "import tellurium as te\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import roadrunner\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.metrics import silhouette_score, pairwise_distances\n",
    "import pandas as pd\n",
    "import time\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "roadrunner.Logger.setLevel(roadrunner.Logger.LOG_CRITICAL)\n",
    "print(\"‚úì All imports loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84984e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "def get_model_path():\n",
    "    linux_path = \"/home/gijs/Documents/OxfordEvolution/Yeast/Chen/chen2004_biomd56.xml\"\n",
    "    mac_path = \"/Users/gijsbartholomeus/Documents/STUDIE/OxfordEvolution/code/Yeast/Chen/chen2004_biomd56.xml\"\n",
    "    \n",
    "    if os.path.exists(linux_path):\n",
    "        return linux_path\n",
    "    elif os.path.exists(mac_path):\n",
    "        return mac_path\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Could not find chen2004_biomd56.xml\")\n",
    "\n",
    "# Configuration\n",
    "model_path = get_model_path()\n",
    "multipliers = [0.25, 0.50, 0.75, 1.00, 1.25, 1.50, 1.75, 2.00]\n",
    "SIMULATION_TIME = 1000\n",
    "N_TIME_POINTS = 50  # Discretize to 50 time points as per paper\n",
    "SAMPLE_SIZE = 100000  # 100K samples for initial testing\n",
    "DIVERGENCE_THRESHOLD = 25\n",
    "\n",
    "# BIRCH parameters\n",
    "BIRCH_THRESHOLD = 0.5  # Distance threshold for clustering\n",
    "BIRCH_BRANCHING_FACTOR = 50\n",
    "BIRCH_N_CLUSTERS = None  # Let BIRCH determine automatically\n",
    "\n",
    "print(f\"‚úì Configuration loaded\")\n",
    "print(f\"   Model: {model_path}\")\n",
    "print(f\"   Sample size: {SAMPLE_SIZE:,}\")\n",
    "print(f\"   Time points: {N_TIME_POINTS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cef60f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CORE FUNCTIONS ===\n",
    "def get_kinetic_parameters(rr):\n",
    "    \"\"\"Get kinetic parameters excluding regulatory switches\"\"\"\n",
    "    kinetic_params = []\n",
    "    \n",
    "    for pid in rr.getGlobalParameterIds():\n",
    "        value = rr.getValue(pid)\n",
    "        param_lower = pid.lower()\n",
    "        \n",
    "        if (param_lower.endswith('t') and value in [0.0, 1.0]) or \\\n",
    "           (param_lower.startswith('d') and param_lower.endswith('n')) or \\\n",
    "           ('flag' in param_lower) or \\\n",
    "           ('switch' in param_lower) or \\\n",
    "           (value == 0.0) or \\\n",
    "           (pid in ['cell']) or \\\n",
    "           ('total' in param_lower and value in [0.0, 1.0]):\n",
    "            continue\n",
    "        else:\n",
    "            kinetic_params.append(pid)\n",
    "    \n",
    "    return kinetic_params\n",
    "\n",
    "def sample_parameters(rr, wildtype=False):\n",
    "    \"\"\"Sample parameters with random multipliers\"\"\"\n",
    "    rr.resetAll()\n",
    "    kinetic_params = get_kinetic_parameters(rr)\n",
    "    \n",
    "    sampled_values = []\n",
    "    \n",
    "    if wildtype:\n",
    "        return [1.0] * len(kinetic_params)\n",
    "    \n",
    "    for pid in kinetic_params:\n",
    "        try:\n",
    "            current = rr.getValue(pid)\n",
    "            factor = random.choice(multipliers)\n",
    "            rr.setValue(pid, current * factor)\n",
    "            sampled_values.append(factor)\n",
    "        except RuntimeError:\n",
    "            sampled_values.append(1.0)\n",
    "    \n",
    "    return sampled_values\n",
    "\n",
    "def simulate_full_network(rr):\n",
    "    \"\"\"Simulate all variables in the network\"\"\"\n",
    "    # Get all species (variables)\n",
    "    species_ids = rr.getFloatingSpeciesIds()\n",
    "    \n",
    "    # Set selections to all species plus time\n",
    "    rr.selections = [\"time\"] + list(species_ids)\n",
    "    \n",
    "    try:\n",
    "        result = rr.simulate(0, SIMULATION_TIME, N_TIME_POINTS + 1)\n",
    "    except RuntimeError:\n",
    "        return None, None\n",
    "    \n",
    "    time_data = result[:, 0]\n",
    "    concentrations = result[:, 1:]  # All species concentrations\n",
    "    \n",
    "    # Check for divergence in any species\n",
    "    if np.any(np.abs(concentrations) > DIVERGENCE_THRESHOLD):\n",
    "        return \"divergent\", None\n",
    "    \n",
    "    # Check for negative concentrations (unphysical)\n",
    "    if np.any(concentrations < 0):\n",
    "        return \"negative\", None\n",
    "    \n",
    "    return species_ids, concentrations\n",
    "\n",
    "print(\"‚úì Core functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9221869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DATA COLLECTION ===\n",
    "def collect_network_data(n_samples=1000):\n",
    "    \"\"\"Collect full network time series data\"\"\"\n",
    "    print(f\"Collecting {n_samples:,} network trajectories...\")\n",
    "    \n",
    "    all_trajectories = []\n",
    "    all_genotypes = []\n",
    "    species_names = None\n",
    "    \n",
    "    success_count = 0\n",
    "    divergent_count = 0\n",
    "    negative_count = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            rate = (i + 1) / elapsed\n",
    "            print(f\"  Progress: {i+1:,}/{n_samples:,} ({rate:.1f}/s) | Success: {success_count:,}\")\n",
    "        \n",
    "        # Load fresh model and sample parameters\n",
    "        rr = te.loadSBMLModel(model_path)\n",
    "        genotype = sample_parameters(rr, wildtype=(i == 0))  # First sample is wildtype\n",
    "        \n",
    "        # Simulate full network\n",
    "        species_ids, concentrations = simulate_full_network(rr)\n",
    "        \n",
    "        if isinstance(species_ids, str):\n",
    "            if species_ids == \"divergent\":\n",
    "                divergent_count += 1\n",
    "            elif species_ids == \"negative\":\n",
    "                negative_count += 1\n",
    "            continue\n",
    "        \n",
    "        if species_ids is None:\n",
    "            divergent_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Store species names from first successful run\n",
    "        if species_names is None:\n",
    "            species_names = species_ids\n",
    "            print(f\"  Network has {len(species_names)} species: {species_names[:5]}...\")\n",
    "        \n",
    "        # Flatten concentrations matrix to create mn-dimensional vector\n",
    "        # Shape: (n_timepoints, m_species) -> (n_timepoints * m_species,)\n",
    "        trajectory_vector = concentrations.flatten()\n",
    "        \n",
    "        all_trajectories.append(trajectory_vector)\n",
    "        all_genotypes.append(genotype)\n",
    "        success_count += 1\n",
    "    \n",
    "    elapsed_total = time.time() - start_time\n",
    "    success_rate = success_count / n_samples * 100\n",
    "    \n",
    "    print(f\"\\n‚úÖ Data collection completed:\")\n",
    "    print(f\"   Success: {success_count:,}/{n_samples:,} ({success_rate:.1f}%)\")\n",
    "    print(f\"   Divergent: {divergent_count:,}\")\n",
    "    print(f\"   Negative: {negative_count:,}\")\n",
    "    print(f\"   Total time: {elapsed_total:.1f}s\")\n",
    "    \n",
    "    if success_count > 0:\n",
    "        trajectory_matrix = np.array(all_trajectories)\n",
    "        print(f\"   Trajectory matrix shape: {trajectory_matrix.shape}\")\n",
    "        print(f\"   (samples √ó (timepoints √ó species): {success_count} √ó ({N_TIME_POINTS+1} √ó {len(species_names)}))\")\n",
    "        \n",
    "        return {\n",
    "            'trajectories': trajectory_matrix,\n",
    "            'genotypes': all_genotypes,\n",
    "            'species_names': species_names,\n",
    "            'success_count': success_count,\n",
    "            'divergent_count': divergent_count,\n",
    "            'negative_count': negative_count\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Collect the data\n",
    "network_data = collect_network_data(SAMPLE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0c0f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === BIRCH CLUSTERING ===\n",
    "if network_data is not None:\n",
    "    print(\"\\nüî¨ Applying BIRCH clustering...\")\n",
    "    \n",
    "    trajectories = network_data['trajectories']\n",
    "    \n",
    "    # Normalize data (important for Euclidean distance)\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    trajectories_normalized = scaler.fit_transform(trajectories)\n",
    "    \n",
    "    print(f\"   Data shape: {trajectories_normalized.shape}\")\n",
    "    print(f\"   Data normalized: mean={np.mean(trajectories_normalized):.3f}, std={np.std(trajectories_normalized):.3f}\")\n",
    "    \n",
    "    # Apply BIRCH clustering\n",
    "    start_time = time.time()\n",
    "    \n",
    "    birch = Birch(\n",
    "        n_clusters=BIRCH_N_CLUSTERS,\n",
    "        threshold=BIRCH_THRESHOLD,\n",
    "        branching_factor=BIRCH_BRANCHING_FACTOR\n",
    "    )\n",
    "    \n",
    "    cluster_labels = birch.fit_predict(trajectories_normalized)\n",
    "    \n",
    "    clustering_time = time.time() - start_time\n",
    "    \n",
    "    # Analyze clustering results\n",
    "    n_clusters = len(np.unique(cluster_labels))\n",
    "    cluster_counts = Counter(cluster_labels)\n",
    "    \n",
    "    print(f\"\\n‚úÖ BIRCH clustering completed:\")\n",
    "    print(f\"   Clustering time: {clustering_time:.1f}s\")\n",
    "    print(f\"   Number of clusters (phenotypes): {n_clusters:,}\")\n",
    "    print(f\"   Largest cluster: {max(cluster_counts.values()):,} genotypes\")\n",
    "    print(f\"   Smallest cluster: {min(cluster_counts.values()):,} genotypes\")\n",
    "    print(f\"   Average cluster size: {np.mean(list(cluster_counts.values())):.1f}\")\n",
    "    \n",
    "    # Calculate silhouette score (if computationally feasible)\n",
    "    if len(trajectories_normalized) <= 10000:  # Only for smaller datasets\n",
    "        silhouette = silhouette_score(trajectories_normalized, cluster_labels)\n",
    "        print(f\"   Silhouette score: {silhouette:.3f}\")\n",
    "    \n",
    "    network_data['cluster_labels'] = cluster_labels\n",
    "    network_data['n_clusters'] = n_clusters\n",
    "    network_data['cluster_counts'] = cluster_counts\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No data collected - skipping clustering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f186d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CLUSTER VALIDATION ===\n",
    "if network_data is not None and 'cluster_labels' in network_data:\n",
    "    print(\"\\nüîç Validating cluster separation...\")\n",
    "    \n",
    "    trajectories_normalized = scaler.transform(network_data['trajectories'])\n",
    "    cluster_labels = network_data['cluster_labels']\n",
    "    \n",
    "    # Sample subset for distance calculations (computational efficiency)\n",
    "    max_samples_for_validation = 5000\n",
    "    if len(trajectories_normalized) > max_samples_for_validation:\n",
    "        indices = np.random.choice(len(trajectories_normalized), max_samples_for_validation, replace=False)\n",
    "        sample_trajectories = trajectories_normalized[indices]\n",
    "        sample_labels = cluster_labels[indices]\n",
    "        print(f\"   Validation on {max_samples_for_validation:,} random samples\")\n",
    "    else:\n",
    "        sample_trajectories = trajectories_normalized\n",
    "        sample_labels = cluster_labels\n",
    "    \n",
    "    # Calculate pairwise distances\n",
    "    distances = pairwise_distances(sample_trajectories, metric='euclidean')\n",
    "    \n",
    "    intra_cluster_distances = []\n",
    "    inter_cluster_distances = []\n",
    "    \n",
    "    for i in range(len(sample_trajectories)):\n",
    "        for j in range(i + 1, len(sample_trajectories)):\n",
    "            if sample_labels[i] == sample_labels[j]:\n",
    "                # Same cluster (intra-cluster)\n",
    "                intra_cluster_distances.append(distances[i, j])\n",
    "            else:\n",
    "                # Different clusters (inter-cluster)\n",
    "                inter_cluster_distances.append(distances[i, j])\n",
    "    \n",
    "    # Statistics\n",
    "    if intra_cluster_distances and inter_cluster_distances:\n",
    "        mean_intra = np.mean(intra_cluster_distances)\n",
    "        mean_inter = np.mean(inter_cluster_distances)\n",
    "        std_intra = np.std(intra_cluster_distances)\n",
    "        std_inter = np.std(inter_cluster_distances)\n",
    "        \n",
    "        separation_ratio = mean_inter / mean_intra\n",
    "        \n",
    "        print(f\"\\nüìä Cluster validation results:\")\n",
    "        print(f\"   Intra-cluster distances: {mean_intra:.3f} ¬± {std_intra:.3f}\")\n",
    "        print(f\"   Inter-cluster distances: {mean_inter:.3f} ¬± {std_inter:.3f}\")\n",
    "        print(f\"   Separation ratio: {separation_ratio:.2f}\")\n",
    "        \n",
    "        if separation_ratio > 1.5:\n",
    "            print(f\"   ‚úÖ Good cluster separation (ratio > 1.5)\")\n",
    "        elif separation_ratio > 1.0:\n",
    "            print(f\"   ‚ö†Ô∏è  Moderate cluster separation (ratio > 1.0)\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Poor cluster separation (ratio ‚â§ 1.0)\")\n",
    "        \n",
    "        network_data['validation'] = {\n",
    "            'mean_intra_distance': mean_intra,\n",
    "            'mean_inter_distance': mean_inter,\n",
    "            'separation_ratio': separation_ratio,\n",
    "            'n_intra_comparisons': len(intra_cluster_distances),\n",
    "            'n_inter_comparisons': len(inter_cluster_distances)\n",
    "        }\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No clustering results - skipping validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12e5d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PHENOTYPE ANALYSIS ===\n",
    "if network_data is not None and 'cluster_labels' in network_data:\n",
    "    print(\"\\nüìà Analyzing phenotype distribution...\")\n",
    "    \n",
    "    cluster_counts = network_data['cluster_counts']\n",
    "    cluster_labels = network_data['cluster_labels']\n",
    "    n_clusters = network_data['n_clusters']\n",
    "    \n",
    "    # Create frequency distribution\n",
    "    frequencies = list(cluster_counts.values())\n",
    "    frequencies.sort(reverse=True)\n",
    "    \n",
    "    # Plot cluster size distribution\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Plot 1: Cluster size histogram\n",
    "    axes[0].hist(frequencies, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0].set_xlabel('Cluster Size (Number of Genotypes)')\n",
    "    axes[0].set_ylabel('Number of Clusters')\n",
    "    axes[0].set_title(f'Cluster Size Distribution\\n{n_clusters:,} clusters total')\n",
    "    axes[0].set_yscale('log')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Rank-frequency plot\n",
    "    ranks = np.arange(1, len(frequencies) + 1)\n",
    "    axes[1].loglog(ranks, frequencies, 'bo-', markersize=3, alpha=0.7)\n",
    "    axes[1].set_xlabel('Phenotype Rank')\n",
    "    axes[1].set_ylabel('Phenotype Frequency')\n",
    "    axes[1].set_title('Rank-Frequency Distribution\\n(Log-log scale)')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Cumulative distribution\n",
    "    cumulative_genotypes = np.cumsum(frequencies)\n",
    "    cumulative_fraction = cumulative_genotypes / network_data['success_count']\n",
    "    axes[2].semilogx(ranks, cumulative_fraction, 'ro-', markersize=3, alpha=0.7)\n",
    "    axes[2].set_xlabel('Number of Most Frequent Clusters')\n",
    "    axes[2].set_ylabel('Fraction of All Genotypes')\n",
    "    axes[2].set_title('Cumulative Genotype Distribution')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    axes[2].axhline(y=0.5, color='gray', linestyle='--', alpha=0.7, label='50% of genotypes')\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('birch_clustering_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nüìä Phenotype distribution summary:\")\n",
    "    print(f\"   Total genotypes analyzed: {network_data['success_count']:,}\")\n",
    "    print(f\"   Total phenotypes (clusters): {n_clusters:,}\")\n",
    "    print(f\"   Largest phenotype: {frequencies[0]:,} genotypes ({frequencies[0]/network_data['success_count']*100:.1f}%)\")\n",
    "    print(f\"   Smallest phenotype: {frequencies[-1]:,} genotypes\")\n",
    "    print(f\"   Singleton phenotypes: {sum(1 for f in frequencies if f == 1):,}\")\n",
    "    print(f\"   Average phenotype size: {np.mean(frequencies):.1f} genotypes\")\n",
    "    print(f\"   Median phenotype size: {np.median(frequencies):.1f} genotypes\")\n",
    "    \n",
    "    # Find number of clusters containing 50% of genotypes\n",
    "    half_genotypes = network_data['success_count'] // 2\n",
    "    for i, cum_count in enumerate(cumulative_genotypes):\n",
    "        if cum_count >= half_genotypes:\n",
    "            clusters_for_half = i + 1\n",
    "            break\n",
    "    \n",
    "    print(f\"   Top {clusters_for_half:,} clusters contain 50% of all genotypes\")\n",
    "    print(f\"   Cluster diversity: {clusters_for_half/n_clusters*100:.1f}% of clusters needed for 50% coverage\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No clustering results - skipping phenotype analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2889b262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EXAMPLE PHENOTYPES ===\n",
    "if network_data is not None and 'cluster_labels' in network_data:\n",
    "    print(\"\\nüé® Visualizing example phenotypes...\")\n",
    "    \n",
    "    cluster_counts = network_data['cluster_counts']\n",
    "    cluster_labels = network_data['cluster_labels']\n",
    "    trajectories = network_data['trajectories']\n",
    "    species_names = network_data['species_names']\n",
    "    \n",
    "    # Get largest and smallest clusters\n",
    "    sorted_clusters = sorted(cluster_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    largest_cluster_id = sorted_clusters[0][0]\n",
    "    smallest_cluster_id = sorted_clusters[-1][0]\n",
    "    \n",
    "    # Pick a medium-sized cluster\n",
    "    medium_cluster_id = sorted_clusters[len(sorted_clusters)//2][0]\n",
    "    \n",
    "    clusters_to_plot = [\n",
    "        (largest_cluster_id, \"Largest\", cluster_counts[largest_cluster_id]),\n",
    "        (medium_cluster_id, \"Medium\", cluster_counts[medium_cluster_id]),\n",
    "        (smallest_cluster_id, \"Smallest\", cluster_counts[smallest_cluster_id])\n",
    "    ]\n",
    "    \n",
    "    # Plot example trajectories from each cluster\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Example Phenotypes: Network Trajectories by Cluster Size', fontsize=16)\n",
    "    \n",
    "    # Select key species to plot (first 3 species)\n",
    "    key_species = species_names[:3]\n",
    "    n_timepoints = N_TIME_POINTS + 1\n",
    "    n_species = len(species_names)\n",
    "    \n",
    "    time_points = np.linspace(0, SIMULATION_TIME, n_timepoints)\n",
    "    \n",
    "    for row, (cluster_id, cluster_type, cluster_size) in enumerate(clusters_to_plot):\n",
    "        # Get indices of genotypes in this cluster\n",
    "        cluster_indices = np.where(cluster_labels == cluster_id)[0]\n",
    "        \n",
    "        # Sample up to 5 trajectories from this cluster\n",
    "        n_examples = min(5, len(cluster_indices))\n",
    "        example_indices = np.random.choice(cluster_indices, n_examples, replace=False)\n",
    "        \n",
    "        for col, species in enumerate(key_species):\n",
    "            species_idx = species_names.index(species)\n",
    "            \n",
    "            for example_idx in example_indices:\n",
    "                # Extract trajectory for this species\n",
    "                trajectory = trajectories[example_idx]\n",
    "                # Reshape to (timepoints, species) and extract specific species\n",
    "                trajectory_matrix = trajectory.reshape(n_timepoints, n_species)\n",
    "                species_trajectory = trajectory_matrix[:, species_idx]\n",
    "                \n",
    "                axes[row, col].plot(time_points, species_trajectory, alpha=0.7, linewidth=1)\n",
    "            \n",
    "            axes[row, col].set_title(f'{cluster_type} Cluster: {species}\\n({cluster_size:,} genotypes)')\n",
    "            axes[row, col].set_xlabel('Time (min)')\n",
    "            axes[row, col].set_ylabel('Concentration')\n",
    "            axes[row, col].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('example_phenotype_trajectories.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"‚úÖ Example phenotype visualizations saved\")\n",
    "    print(f\"   Largest cluster ({largest_cluster_id}): {cluster_counts[largest_cluster_id]:,} genotypes\")\n",
    "    print(f\"   Medium cluster ({medium_cluster_id}): {cluster_counts[medium_cluster_id]:,} genotypes\")\n",
    "    print(f\"   Smallest cluster ({smallest_cluster_id}): {cluster_counts[smallest_cluster_id]:,} genotypes\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No clustering results - skipping example visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183fa8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FINAL SUMMARY ===\n",
    "if network_data is not None:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MULTI-VARIABLE PHENOTYPE CLUSTERING SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nüî¨ DATASET OVERVIEW:\")\n",
    "    print(f\"   Samples attempted: {SAMPLE_SIZE:,}\")\n",
    "    print(f\"   Successful simulations: {network_data['success_count']:,} ({network_data['success_count']/SAMPLE_SIZE*100:.1f}%)\")\n",
    "    print(f\"   Network species: {len(network_data['species_names'])}\")\n",
    "    print(f\"   Time points: {N_TIME_POINTS + 1}\")\n",
    "    print(f\"   Feature dimensions: {len(network_data['species_names']) * (N_TIME_POINTS + 1):,}\")\n",
    "    \n",
    "    if 'cluster_labels' in network_data:\n",
    "        print(f\"\\nüéØ CLUSTERING RESULTS:\")\n",
    "        print(f\"   Phenotypes discovered: {network_data['n_clusters']:,}\")\n",
    "        print(f\"   BIRCH threshold: {BIRCH_THRESHOLD}\")\n",
    "        print(f\"   Branching factor: {BIRCH_BRANCHING_FACTOR}\")\n",
    "        \n",
    "        frequencies = list(network_data['cluster_counts'].values())\n",
    "        print(f\"   Largest phenotype: {max(frequencies):,} genotypes\")\n",
    "        print(f\"   Smallest phenotype: {min(frequencies):,} genotypes\")\n",
    "        print(f\"   Average phenotype size: {np.mean(frequencies):.1f} genotypes\")\n",
    "        \n",
    "        singleton_count = sum(1 for f in frequencies if f == 1)\n",
    "        print(f\"   Singleton phenotypes: {singleton_count:,} ({singleton_count/network_data['n_clusters']*100:.1f}%)\")\n",
    "        \n",
    "        if 'validation' in network_data:\n",
    "            val = network_data['validation']\n",
    "            print(f\"\\n‚úÖ CLUSTER VALIDATION:\")\n",
    "            print(f\"   Separation ratio: {val['separation_ratio']:.2f}\")\n",
    "            print(f\"   Intra-cluster distance: {val['mean_intra_distance']:.3f}\")\n",
    "            print(f\"   Inter-cluster distance: {val['mean_inter_distance']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nüìä GENOTYPE-PHENOTYPE MAPPING:\")\n",
    "    if 'cluster_labels' in network_data:\n",
    "        total_genotypes = network_data['success_count']\n",
    "        total_phenotypes = network_data['n_clusters']\n",
    "        redundancy = total_genotypes / total_phenotypes\n",
    "        \n",
    "        print(f\"   Many-to-one ratio: {redundancy:.1f} genotypes per phenotype (average)\")\n",
    "        print(f\"   Phenotype diversity: {total_phenotypes/total_genotypes*100:.2f}% unique phenotypes\")\n",
    "        \n",
    "        # Calculate entropy\n",
    "        frequencies = np.array(list(network_data['cluster_counts'].values()))\n",
    "        probabilities = frequencies / total_genotypes\n",
    "        entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "        max_entropy = np.log2(total_phenotypes)\n",
    "        normalized_entropy = entropy / max_entropy\n",
    "        \n",
    "        print(f\"   Phenotype entropy: {entropy:.2f} bits ({normalized_entropy:.2f} normalized)\")\n",
    "    \n",
    "    print(f\"\\nüìÅ OUTPUT FILES:\")\n",
    "    print(f\"   ‚Ä¢ birch_clustering_analysis.png - Cluster distribution plots\")\n",
    "    print(f\"   ‚Ä¢ example_phenotype_trajectories.png - Example phenotype trajectories\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANALYSIS COMPLETED SUCCESSFULLY! üéâ\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ùå Analysis failed - no successful simulations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioevo_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
